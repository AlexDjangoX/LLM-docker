version: '3.8'

services:
  # API Gateway - Node.js/Express backend
  llm-services:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-services
    ports:
      - "${PORT:-3000}:3000"
    environment:
      - PORT=3000
      - NODE_ENV=production
      # Self-hosted AI service URLs (Docker internal network)
      - OLLAMA_BASE_URL=http://ollama:11434
      - STABLE_DIFFUSION_URL=http://stable-diffusion:7860
      - XTTS_URL=http://xtts:80
      - LIBRETRANSLATE_URL=http://libretranslate:5000
      # Security - Frontend URL for CORS
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-http://localhost:3001}
      - RATE_LIMIT_MAX=${RATE_LIMIT_MAX:-100}
    restart: unless-stopped
    networks:
      - llm-network
    # Start after AI services are up (but don't require healthy - allows graceful degradation)
    depends_on:
      ollama:
        condition: service_started
      xtts:
        condition: service_started
      libretranslate:
        condition: service_started
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ============================================================================
  # OLLAMA - Self-hosted LLM for Chat & Educational Tutoring
  # ============================================================================
  # Recommended model for bilingual education (requires ~8GB RAM):
  #   docker exec ollama ollama pull llama3.1:8b
  # Alternative lighter model (~4GB RAM):
  #   docker exec ollama ollama pull llama3.2:3b
  # ============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - llm-network
    healthcheck:
      # Ollama image has curl installed
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    # GPU support - uncomment if you have NVIDIA GPU
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # ============================================================================
  # STABLE DIFFUSION - Image Generation (DISABLED - Optional)
  # ============================================================================
  # Note: Stable Diffusion requires GPU for practical use and large downloads.
  # For CPU-only systems, image generation is very slow (5-30 min per image).
  # Uncomment this section if you have an NVIDIA GPU and want image generation.
  # ============================================================================
  # stable-diffusion:
  #   image: ghcr.io/ai-dock/stable-diffusion-webui:latest
  #   container_name: stable-diffusion
  #   ports:
  #     - "7860:7860"
  #   volumes:
  #     - stable-diffusion-data:/workspace
  #   environment:
  #     - WEBUI_FLAGS=--api --listen --port 7860
  #   restart: unless-stopped
  #   networks:
  #     - llm-network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # ============================================================================
  # XTTS STREAMING SERVER - Professional Neural Text-to-Speech (Multilingual)
  # ============================================================================
  # XTTS v2: Natural multilingual TTS in 17+ languages (Polish, English, etc.)
  # API: POST /tts_to_audio with JSON body {text, speaker_wav, language}
  # First run downloads model (~1.8GB), subsequent requests use cache
  # Note: CPU mode is slower (~30s per request) but works without GPU
  # ============================================================================
  xtts:
    image: ghcr.io/coqui-ai/xtts-streaming-server:latest-cpu
    container_name: xtts
    ports:
      - "8000:80"
    volumes:
      - xtts-data:/app/tts_models
    environment:
      - COQUI_TOS_AGREED=1
    restart: unless-stopped
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:80/languages"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # 5 min for model download on first run

  # ============================================================================
  # LIBRETRANSLATE - Professional Translation Engine (EN â†” PL)
  # ============================================================================
  # Dedicated translation service optimized for accuracy
  # Supports: English, Polish, and 20+ other languages
  # API: POST /translate with {q: "text", source: "en", target: "pl"}
  # Test: curl -X POST http://localhost:5000/translate -d '{"q":"Hello","source":"en","target":"pl"}'
  # ============================================================================
  libretranslate:
    image: libretranslate/libretranslate:latest
    container_name: libretranslate
    ports:
      - "5000:5000"
    volumes:
      - libretranslate-data:/home/libretranslate/.local
    environment:
      # Pre-load English and Polish language models
      - LT_LOAD_ONLY=en,pl
      # Disable rate limiting for internal use
      - LT_DISABLE_WEB_UI=false
      - LT_UPDATE_MODELS=true
    restart: unless-stopped
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:5000/languages"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s  # 3 minutes for language model downloads

networks:
  llm-network:
    driver: bridge
    name: llm-network

volumes:
  ollama-data:
    name: ollama-data
  xtts-data:
    name: xtts-data
  libretranslate-data:
    name: libretranslate-data
  # Uncomment if using Stable Diffusion:
  # stable-diffusion-data:
  #   name: stable-diffusion-data
