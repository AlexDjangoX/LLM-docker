version: '3.8'

services:
  llm-services:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-services
    ports:
      - "${PORT:-3000}:3000"
    environment:
      - PORT=3000
      - NODE_ENV=production
      # Self-hosted providers
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - STABLE_DIFFUSION_URL=${STABLE_DIFFUSION_URL:-http://stable-diffusion:7860}
      - OPENTTS_URL=${OPENTTS_URL:-http://opentts:5500}
      # Security
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-}
      - RATE_LIMIT_MAX=${RATE_LIMIT_MAX:-100}
    env_file:
      - .env
    restart: unless-stopped
    networks:
      - llm-network
    depends_on:
      ollama:
        condition: service_healthy
      stable-diffusion:
        condition: service_healthy
      opentts:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama - Self-hosted LLM (FREE, runs on your Hetzner server)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    # GPU support (uncomment if you have NVIDIA GPU on Hetzner)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # RunPod Stable Diffusion - Reliable and fast image generation
  stable-diffusion:
    image: runpod/stable-diffusion:web-automatic
    container_name: stable-diffusion
    ports:
      - "7860:7860"
    volumes:
      - stable-diffusion-models:/stable-diffusion-webui/models/Stable-diffusion
      - stable-diffusion-outputs:/stable-diffusion-webui/outputs
    environment:
      - WEBUI_ARGS=--api --listen --port 7860 --no-half --skip-torch-cuda-test --enable-insecure-extension-access --no-download-sd-model
    restart: unless-stopped
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 300s
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # OpenTTS - HTTP API for Piper TTS (simple and reliable)
  opentts:
    image: synesthesiam/opentts:latest
    container_name: opentts
    ports:
      - "5500:5500"
    environment:
      - TTS__PIPE__LENGTH_SCALE=1.0
      - TTS__PIPE__NOISE_SCALE=0.667
      - TTS__PIPE__NOISE_W=0.8
      - TTS__PIPE__SPEAKER_EMBEDDING_DIMENSION=256
    volumes:
      - opentts-models:/app/models
    restart: unless-stopped
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5500/api/tts?text=test"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

networks:
  llm-network:
    driver: bridge

volumes:
  ollama-data:
  stable-diffusion-models:
  stable-diffusion-outputs:
  opentts-models:
