version: '3.8'

services:
  llm-services:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-services
    ports:
      - "${PORT:-3000}:3000"
    environment:
      - PORT=3000
      - NODE_ENV=production
      # Self-hosted providers
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - LOCALAI_BASE_URL=${LOCALAI_BASE_URL:-http://localai:8080}
      - OPENTTS_URL=${OPENTTS_URL:-http://opentts:5500}
      # Security
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-}
      - RATE_LIMIT_MAX=${RATE_LIMIT_MAX:-100}
    env_file:
      - .env
    restart: unless-stopped
    networks:
      - llm-network
    depends_on:
      - ollama
      - localai
      - opentts
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Ollama - Self-hosted LLM (FREE, runs on your Hetzner server)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - llm-network
    # GPU support (uncomment if you have NVIDIA GPU on Hetzner)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # LocalAI with image generation support
  localai:
    image: localai/localai:latest-aio-cpu
    container_name: localai
    ports:
      - "8080:8080"
    volumes:
      - localai-models:/models
    environment:
      - MODELS_PATH=/models
    restart: unless-stopped
    networks:
      - llm-network
    # GPU support (uncomment if you have NVIDIA GPU)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # OpenTTS - HTTP API for Piper TTS (simple and reliable)
  opentts:
    image: synesthesiam/opentts:latest
    container_name: opentts
    ports:
      - "5500:5500"
    restart: unless-stopped
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge

volumes:
  ollama-data:
  localai-models:
